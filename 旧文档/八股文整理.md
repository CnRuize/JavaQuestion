
## Java 动态代理
​ Java 的动态代理可以分解为两种：JDK 动态代理和 CGLib 动态代理。由于 CGLib 是在 JDK7 之前是代理速度更快一些，因此简短描述。
1. JDK 动态代理**
​JDK 的动态代理是基于方法接口来实现的
```java
Proxy.newProxyInstance(JdkDynamic.class.getClassLoader(),new Class[]{ eat.class },new MyInvocationHandler());
```
​上述三个参数分别是"类加载器"，"代理的接口列表"，"重写的 InvocationHandler（请求处理者）"
​这里着重解释第三部分"InvocationHandler"，这个类似于一个"过滤器"，但是过滤的并不是请求，而是"调用的方法"，再调用方法之前会首先执行重写的 Handler 中的方法，然后再去执行我们调用的方法，类似于过滤器。
2. CGLib 代理**
​CGLib 代理是基于类的继承实现的，是类代理，在没有接口的时候使用，代理的速度在 JDK7 以下是比较快的。由于代理是基于继承实现的，所以不能代理 final 类跟 final 方法以及 private 方法。

## 常见的设计模式

设计模式分为三种：创建型模式，结构型模式，行为型模式

1. 创建型模式

   - 单例模式：确保一个类只有一个实例，仅提供一个全局访问点，比如需要一个记录日志的，仅仅希望被创建一次，采用静态内部类的方法来实现。

     - ```java
       // LoggerManager.java
       public class LoggerManager {

           // 私有构造方法，防止外部new
           private LoggerManager() {
               System.out.println("LoggerManager 构造方法被调用！");
           }

           // 静态内部类，懒加载 + 线程安全
           private static class Holder {
               private static final LoggerManager INSTANCE = new LoggerManager();
           }

           // 提供全局唯一访问入口
           public static LoggerManager getInstance() {
               return Holder.INSTANCE;
           }

           // 示例日志方法
           public void logInfo(String message) {
               System.out.println("[INFO] " + message);
           }

           public void logError(String message) {
               System.err.println("[ERROR] " + message);
           }
       }
       // MainApp.java
       public class MainApp {
           public static void main(String[] args) {
               System.out.println("程序启动...");

               // 模拟模块1记录日志
               LoggerManager logger1 = LoggerManager.getInstance();
               logger1.logInfo("模块1：初始化完成");

               // 模拟模块2记录日志
               LoggerManager logger2 = LoggerManager.getInstance();
               logger2.logError("模块2：发生了异常");

               // 比较是否是同一个对象
               System.out.println("两个logger是否相同: " + (logger1 == logger2));
           }
       }
      ```


       这里涉及到了类加载重新复习了下：

       启动 MainApp → 加载 MainApp 类
       → 输出"程序启动..."
       → 调用 LoggerManager.getInstance()
       → 加载 LoggerManager
       → 访问 Holder.INSTANCE → 加载 Holder
       → 初始化 INSTANCE = new LoggerManager()
       → 构造方法执行
       → logger1.logInfo() → 输出日志
       → logger2 = getInstance() → 直接返回已存在对象
       → logger2.logError() → 输出日志
       → 比较 logger1 == logger2 → 输出 true

   - 工厂方法模式

   - 抽象工厂模式


### Spring

#### SpringMVC 的执行流程

1. 客户端向服务器发出 Http 请求之后，首先会被 DispatcherServlet 接收
2. DispatcherServlet 会调用 HandlerMappering 根据请求的地址来找到处理该请求的 Controller
3. 通过 HandlerAdapter 调用具体的 Controller
4. Controller 业务处理
5. 处理完成之后返回 ModelAndView 对象，如果加了 ResponseBody 则是直接返回 JSON 数据。
6. 最后分为两种情况，一是返回的 ModelAndView，这个时候 DispatcherServlet 会调用 ViewResolver 来解析 View 数据，并把 model 数据填充到视图中；如果返回的是 Json 对象的话则直接返回给前端即可
### Mysql

#### Mysql 如何定位慢查询

1. 可以使用运维工具比如 Skywalking，可以看到具体某个接口执行情况
2. 使用 Mysql 慢日志的查询，设置 SQL 执行时间，比如说当前 sql 执行超过两秒就记录当前 sql

#### SQL 执行很慢如何优化

1. 是否出现了聚合查询，多表查询，查询的数据量是不是很大？

2. 可以使用 Explain 字段来分析对应的 sql，返回的结果中会有几个需要关注的字段

   1. key 和 possible_key，这可以确定当前 sql 使用了哪个索引

   2. extra 字段，这是 mysql 自己给出的建议，这里可以看出当前的 sql 是否需回表

      | extra 内容      | 含义解释                                         |
      | --------------- | ------------------------------------------------ |
      | Using index     | 使用了**覆盖索引**，不需要回表                   |
      | Using where     | 有使用 `WHERE` 条件过滤                          |
      | Using temporary | 用了临时表，通常和 `GROUP BY` 或 `ORDER BY` 有关 |
      | Using filesort  | 表示排序是**通过外部排序实现的**，不是索引排序   |
      | **NULL**        | 表示没有额外操作，通常是个好事（查询很干净）     |

   3. type 字段表示的是 Mysql 访问数据的方式性能从好到坏一般为：

      ```sql
      system > const > eq_ref > ref > range > index > all
      ```

      1. system：表示只有一行数据
      2. const：通过主键或者是唯一索引等值查询，直返会一条结果
      3. eq_ref：每次扫描主表时，子表通过主键或者是唯一索引精确匹配一条记录
      4. ref：通过非唯一索引匹配到若干条记录
      5. range：使用了范围查询
      6. index：遍历了整个索引，索引的全扫描
      7. all：全表扫描，

#### Mysql 索引

##### 索引是什么？

​ 索引是为了提高查询效率所建立的一种数据结构组织形式，在不同的数据库中有不同的表现方式，比如说在 Mysql 数据库中常见的 Innodb 是采用 B+树的方式实现的，对于旧版本的 MyISAM 引擎也曾采用 B+树来实现，但是聚簇机制不同。

##### 索引的基本类型

Mysql 索引类型包含以下几种：

1. 主键索引（字段值不可以重复）

2. 普通索引（索引字段的值可以重复）

3. 唯一索引（字段值不可以重复）

4. 联合索引（两个字段一起作为一个索引）

   河北+小明 != 安徽+小明

5. 前缀索引（取当前字段的前几个字母）

6. 全文+空间索引（暂时用不到，忽略）

只有主键索引在 B+树中是保存了完整的行数据。

##### 索引覆盖

要查询的字段内容是 B+树叶子结点的一部分

- ​ 比如说我们查询全部内容采用主键索引作为依据，这个就是索引覆盖，因为主键索引对应的叶子结点是整行完整数据。
- ​ 如果采用非主键索引，得到结果是只有当前行的主键以及普通索引的值，如果我们要的字段没有那么就要回表

#### 索引创建的原则

- 针对数据量特别大的表（10 万条数据以上）；
- 经常作为查询条件的字段，一个或者是多个；
- 在区分度高的字段上创建索引，可以仅仅通过索引就可以定位具体的信息
- 如果是字符串形式的比较长，可以用前缀索引，取字符串的前几个字符
- 尽量使用联合索引——>便于索引覆盖
- 索引并不是越多越好——>上楼（步行，电梯，飞机）方式举例

#### 索引失效

- 违反最左索引原则（这个情况只会在多字段索引出现）
  - 如果遵从最左索引，但是跳跃了某一列，这样的话就只有最左索引生效
- 范围查询右边的列，不能使用索引
  - 比如果三个字段的索引，第一个字段是"="，第二个字段是">"，第三个字段不管是什么都会失效
- 不能在索引上进行运算操作
  - 比如说对姓名字段进行截取
- 类型转换失败
  - 当前字段是数字类型，传递值是字符串类型呢
- 以%开始的模糊匹配
  - 如果是放在最后边做模糊匹配的话就可以

#### sql 优化经验

1. select 禁止使用 select \*（为了使用覆盖索引，降低回表查询）
2. SQL 语句尽量不要写成索引失效的方法
   1. 避免在 where 字句中对字段进行表达式操作
3. 在联合查询结果的时候使用 union all 来代替 union
   1. 使用 union all 会直接返回两个结果的合集
   2. 使用 union 的话会将重复部分仅返回一个
4. 在进行连接查询的时候尽量使用内连接，而不是用左右连接
   1. 内连接仅仅会返回两个表中都满足匹配标准的内容
   2. LEFT/RIGHT JOIN 会保留主表所有记录，若副表存在一对多关系，极易造成结果集膨胀，甚至影响后续分页、聚合逻辑。

#### mysql 为什么使用 Innodb 作为数据库引擎

1. 支持事务（ACID，原子性，一致性，隔离性，持久性）
   1. 这里就用存钱取钱的例子来记忆，原子性表示这个过程不可以分割，比如说存钱取钱一起作为一个事务，那么就表示要么一起完成要么就失败回滚
2. 行级锁
   1. 相对于 Myisam 而言，行级锁比表级锁的性能要更高
3. MVCC 多版本并发控制
4. 崩溃恢复
   1. 基于重做日志（redo log）以及回滚日志（undo log）
5. 支持外键
6. 聚簇索引

#### Mysql 的超大分页处理

​ 分页查询常使用 `LIMIT A, B` 语法，表示从第 A 条数据开始，查询 B 条记录。其本质执行过程决定了性能的好坏，尤其是在数据量很大时。首先需要了解一下这个过程为什么会耗费资源，在默认情况下 limit 会将前 n 条数据都缓存在内存中，如果有索引的话那么缓存的数据是 B+树路径上的所有数据，数据还少一些。

- 换成 where id（主键）> n limit 50 的方式

  - 主键范围查询可以实现从 B+树中快速定位所要的数据位置，不需要把数据缓存在内存

- 使用主键的 order by 来操作

  - 这个优化不如上边，主要消耗在线性遍历链表，这里需要了解 sql 的执行流程，索引的 order by 来举例：

  - ```sql
    SELECT * FROM user ORDER BY id LIMIT 90000, 50;
    ```

  - 这里因为 id 是主键，Mysql 会从 B+树种由根节点找到最左叶子结点，然后通过叶子结点之间的双向链表来进行链表的线性遍历，直到 90000 这个数据，然后往后读取 50 条，返回结果

#### Mysql 查询

##### 聚合查询

聚合查询通过 group by 来实现，目的是统计某一项信息

```sql
-- 统计每个用户的订单总金额
SELECT user_id, SUM(order_amount)
FROM orders
GROUP BY user_id;
```

聚合索引会查询很慢往往是因为数据量很大，Mysql 内存临时表放不下，然后要写入磁盘，这会导致性能极致下降。

##### 多表查询

通过键的联合来形成一张逻辑表

```sql
-- 查询每个订单的用户姓名和订单金额
SELECT o.id, o.amount, u.username
FROM orders o
JOIN users u ON o.user_id = u.id;
```

多表查询+聚合查询示例

```sql
-- 查询每个用户的总消费金额（需要先关联订单再聚合）
SELECT u.username, SUM(o.amount) AS total_spent
FROM users u
JOIN orders o ON u.id = o.user_id
GROUP BY u.id, u.username;
```

##### 深度分页查询

深度分页查询就是分页去查询很后边的数据。

```sql
SELECT *
FROM your_table
ORDER BY id
LIMIT 89990, 10;
```

上述 sql 就是从 89990 条数据开始，查询十条数据。

#### Mysql 事务的特性

​ ACID

1. 原子性
2. 一致性
3. 持久性
4. 隔离性

#### Mysql 事务隔离级别

##### 问题引入：并发事务带来哪些问题？

​ 脏读，不可重复，幻读

​ 脏读：一个事务读取到另一事务还未提交的数据

​ 不可重复度：一个事务先后读取同一条数据，两次读取数据不同

​ 幻读：一个事务在查询时没有查询到数据，但是插入数据时显示当前数据已经存在

##### Mysql 事务隔离级别解决并发事务问题

​ 事务隔离级别有读未提交，读已提交，可重复读，串行化

​ 读未提交：什么也解决不掉

​ 读已提交：解决脏读问题

​ 可重复读：解决了脏读和不可重复读的问题；这是 Mysql 默认的事务隔离级别

​ 序列化：全部解决，性能降低

#### 默认的隔离级别可重复读是如何实现的？

#### undo log 和 redo log 的区别

undo log 记录的是逻辑日志，主要用于事物的回滚，支持事务的原子性

redo log 记录的是数据物理页面的变化，服务器宕机之后可以用来恢复数据

​ 在数据写入的时候 Inoodb 会先写入 redo log 然后再写入磁盘

#### MVCC 是什么？

MVCC 是 Mysql 提供的一种多版本控制的机制

### Redis

#### Redis 为什么这么快

1. Redis 将数据存储在内存中，内存速度比磁盘快。
2. Redis 提供高效的数据结构。
3. Redis 使用单线程事件驱动模型，结合**IO 多路复用**，避免了多线程上下文切换条件，提高了并发处理效率。

1、**I/O 多路复用？**

​ Redis 是纯内存操作，执行速度很快，瓶颈在于网络延迟，I/O 多路复用主要实现高效的网络请求。IO 多路复用是用单个线程来同时监听多个 Socket，并在某个 Socket 可读可写的时候得到通知，避免无效的等待。

1. 同步阻塞：逐个收作业，A 没做完就等 A 做完再收其他人的。
2. 同步非阻塞：逐个收作业，不过 A 没做完就暂时不管 A，先收别人的。
3. select/poll：有人做完作业会举手，但是你并不知道谁举手，需要一个一个问。
4. epoll：有人做完就举手，你也知道谁做完了。

#### Redis 可以保存多少种数据类型（5 种常见+3 种特殊）

1. String

2. Hash

   - redis 内部使用哈希表来实现

   - 其实存储的就是键值对，不过这个值可以是数组类型的，像一个类一样。

     ![Redis哈希值](C:\Users\Ruize\Desktop\文档\Java学习\Java八股文整理\Markdown文档图片\Redis哈希值.png)

3. List

   ![List类型数据](C:\Users\Ruize\Desktop\文档\Java学习\Java八股文整理\Markdown文档图片\List类型数据.png)

4. Set

   和一般的 Set 没什么区别，都是没有重复数据，不过对应的方法有所变化。

5. ZSet

   数据在插入时都会附带一个参数，数据的顺序按照，按照分数从低到高排序。

   ![set](C:\Users\Ruize\Desktop\文档\Java学习\Java八股文整理\Markdown文档图片\set.png)

   6.BitMap 位图

   存储的也是键值对，不过键是分级的，值的保存形式是一个字符串，设置好偏移位，并设置对应的值 1 或者是 0 即可

   setbit ruize：20250510 1 1；键是 ruize，字键是 20250510 ，偏移位 1，值是 1

   7.Hyperloglog

   内从占用低，即使有 1 亿个数据，内存占用也是 12kb，不能查看其中的内容，只是可以数据统计得到其中有多少唯一的内容

   8.Stream 流

   有点类似于消息队列

   ```shell
   # 1. 创建Stream并添加事件（无需预先声明Stream）
   XADD payment_events * user_id "101" amount "99.99" status "pending"

   # 2. 创建消费者组（绑定到payment_events流）——>某个流下包含多个组
   XGROUP CREATE payment_events payment_group $

   # 3. 消费者A从组中读取事件
   XREADGROUP GROUP payment_group consumerA COUNT 1 STREAMS payment_events >

   # 4. 处理完成后确认
   XACK payment_events payment_group 1715581234567-0
   ```

#### Redis 缓存穿透，雪崩，击穿

1. 缓存穿透：查询一个不存在的数据，导致请求访问数据库，
   - 解决：
     - 缓存这个不存在的数据
     - 使用布隆过滤器，过滤器中包含数据库中的全部内容
       - 存储数据时，通过多个哈希函数来进行运算，把数组中对应的位置改为 1，在进行查询的时候，对查询数据进行哈希运算，检查结果位置是否全部为 1.
2. 缓存雪崩：大量缓存数据在同一时间过期导致请求大量访问数据库
   - 解决
     - 使用随机过期时间
     - 更新频率提高，不要等到缓存过期采取更新
3. 缓存击穿：某个热点数据过期导致请求大量访问数据库
   - 解决
     - 热点数据永远不过期
     - 使用互斥锁，确保同一时间只有一个请求可以去数据库查询数据

#### Redis 主从复制

​ 从节点向主节点发送 Psync 命令来建立一个链接

**1、全量同步**

​ 如果是第一次连接或者是之前的连接失效，主节点会将数据快照 RDB 全部发送给从节点

​ 1.1 过程解析

​ 从节点发送 psync replicationID offset 给主节点，replicationID 为服务器当前的的 replicationID ，由于第一次同步并不知道主服务器的 replicationID 是多少，所以这里是" ？"，而对于 offset 复制进度来讲，第一次同步的值为-1，所以综上所述第一次从节点进行全量同步时，发送的命令为 psync ？ -1；主节点收到命令之后发现 replicationID 没有值，于是确定本次复制为全量复制，主节点会生成 RDB 文件，在 RDB 文件生成过程中如果发生了新的写的操作，把新的写操作保存在 repilcation buffer 中等待从节点 RDB 文件加载完成之后把 replication buffer 文件中存储的数据发送给从节点，从节点继续把数据写入，实现数据一致性。

**2、增量同步**

​ 全量复制完毕之后，主节点和从节点之间还会保持一个长连接，当主节点完成写操作之后，会相应的更新从节点

​ 2.1 过程解析

​ 当全量同步一次之后断开了，此时再进行全量同步的话消耗比较大。repl_backlog_buffer：环形缓冲区，默认大小为 1M，主节点会将命令缓存到当前的缓冲区，但是超过 1M 之后，之前的数据就会被覆盖。对于增量同步而言，命令也是 psync，如果主节点根据 replicationID 判断和主节点一致而且根据 offset 判断数据还在 repl_backlog_buffer 中就进行增量同步，在 repl_backlog_buffer 中的数据写入到 replicaiton buffer 中，最后将 replication buffer 写入给子节点，一次增量同步完成。

**3、repl_backlog_buffer 和 replication buffer 的区别**

​ repl_backlog_buffer 仅仅有一份但是 replication buffer 对于每一个子节点都有一份。

​ 3.1 优点

​ 主从复制可以实现读写分离，写数据操作在主节点实现，读操作在从节点实现。

**4、问题**

1. 如果刚在主节点写好，还未来得及同步给从节点，这个时候从节点是读取不到数据的，要访问数据库了吗？
1. 不要，直接返回 null 值

#### Mysql 中的数据如何与 Redis 进行同步呢？（双写一致性）

一致性要求高

​ 1、采用写锁的方案来实现，当用户查询 Redis 中发现不存在的时候，查询数据库，得到结果之后先获取写锁，保证只有当前线程可以对 Redis 进行读取和写数据的操作，保证数据一致性。

​ 2、采用 canal 来实现，canal 是阿里巴巴开源的中间件，他会解析"Binary log"记录 Mysql 数据库中的变化，解析其中的内容，然后直接调用 Redis 的 API 更新其中的数据，实现强一致性。

双写一致性基本方案有如下几种：

1. 先更新缓存，再更新数据库

   1. A 线程更新缓存，下一步更新数据库，此时 B 线程查询缓存，得到正确数据，下一步 A 线程更新数据库，最终结果是得到的数据是正确的，缓存跟数据库中的数据也是正确的。
      1. 异常情况：数据库更新失败，B 第一次得到的数据是正确的，但是如果 A 更新数据失败，比如说死锁或者是遇到了网络问题，Mysql 数据回滚了，这样就会导致缓存中是异常的值，与 Mysql 中的数据并不一致。

2. 先删除缓存，再更新数据库

   1. A 线程删除缓存，下一步更新数据库，此时 B 线程查询不到缓存来查询数据库，得到的旧的数据，返回更新缓存，缓存中的数据是旧的数据，A 回头来更新数据库，最终结果是数据库内容正确，缓存内容不正确。

3. 先更新数据库，再更新缓存

   1. 存在脏数据问题，数据库中数据已经更新了，缓存还没来得及更新，此时第二个线程此时查询的是缓存中的数据，脏数据问题，最后更新缓存，最终结果是数据库和缓存都是正确的，但是在这期间出现了脏数据的问题。

4. 先更新数据库，再删除缓存（推荐 √）

   1. 存在脏数据问题，数据库中数据已经更新了，缓存还没来得及删除，此时第二个线程此时查询的是缓存中的数据，脏数据问题，最后重新建立缓存，最终结果是数据库缓存都正确，但是期间出现了脏数据的问题。

5. 先删除缓存，更新数据库，删除缓存（延迟双删）

   执行流程：删除缓存，更新数据库，延时 300-800ms 再次删除缓存

   ​ 1. A 线程删除缓存，该更新数据库了，B 线程查询数据得到脏数据，更新缓存，A 线程更新数据库，延时删除缓存，最终结果是数据库内容正确，但是过程中 B 线程得到了脏数据。

   ​ 2. A 线程删除缓存，更新数据库，此时 B 来查询就是得到的正确数据，A 删除缓存，最终结果就是全对。

​ 6. 结合消息队列（延时双删改版）——>最终一致性，还是存在脏数据问题

​ 第二次删除使用消息队列发送消息给 Redis，确定多长时间之后进行缓存的删除，同时可以设置消息重传机制以及死信队列。

​ 7. 采用 redis 的分布式锁来实现强一致性 ——>强一致性

​ redis 的分布式锁控制的是线程先后顺序，可以确保在拿到锁的时间段内，只有当前线程可以操作。

​ 8. 采用阿里巴巴开源组件 cannal

​ 这是一种强一致性的方案，这是一个基于 MySQL binlog 解析的增量数据订阅&同步组件，Canal 模拟成 Mysql 的从库，监听 Mysql 的 binlog 日志，实时捕获数据变更并更新到 Redis，使用要求有以下几点：

1. Mysql 必须要开启 Binlog 日志，格式必须要是 Row 格式
2. Canal 解析 Binlog 文件，解析数据变更。
3. Canal 触发回调，将变更同步到 Redis
4. Redis 与 Mysql 保持数据一致

使用方法有点难，搞了半天没弄成。

​ 分布式锁：分布式锁是 redis 专属的一种锁，线程首先获取锁，因为 SET NX PX 的原因，其中 NX 表示不存在，如果不存在就就获取锁，存在就放弃，所以这就可以确保只有一个线程可以操作。

#### Redis 持久化

1. AOF

   Append-Only File Redis 执行的每一条命令都会写入到 AOF 文件中，

   ​ 优点：数据完整性好，可以通过配置来决定 AOF 的写入频率

   ​ 缺点：写操作频繁，性能比较差

2. RDB

   Redis DataBase Backup file Redis 数据快照，把内存中的数据都存储在磁盘中，当数据恢复时会从磁盘读取所有数据。

   Redis 在生成快照的时候会采用 fork 机制，开启一个子进程执行持久化，主进程不会受到阻塞。

   ​ 缺点：生成快照之前的数据会丢失。

   RDB：比如 300 秒内写入 5 次就进行 RDB，这里的时间间隔是距离上一次 RDB 完成之后的时间

#### 数据的删除策略

1. 惰性删除：使用 key 时会检测其是否过期，过期的话就删除，没过期就拉倒
2. 定期删除：每隔一段时间就对一定量的 Key 进行检测，过期的删除，没有过期不管

#### Redis 数据淘汰策略

1. 默认策略是不淘汰任何 key，内存满的时候不允许写入数据
2. volatile-ttl（ [ˈvɒlətaɪl]）：TTL 剩余值越小的先被淘汰
3. allkeys-random：所有的 key 随机淘汰
4. volatile-random:对设置过期时间的 key 随机进行淘汰
5. allkeys-lru:对全体 key 使用 LRU（Least Recently Used 最近最少未使用）算法淘汰：用当前时间减去最后一次访问时间，值越大越先被淘汰
6. volatile-lru：对设置了过期时间的 key 采用 lru 算法
7. allkeys-lfu:对全体 key 使用 LFU（Least Frequently Used 最少频率使用）会统计每个值的访问频率，值越小，越先被淘汰。
8. volatile-lfu：对设置过期时间的 key 采用 lfu 算法进行淘汰

#### Redis 的分布式锁

​ 分布式锁：在锁存在期间，仅有当前获得锁的线程可以进行输出读写操作。

​ Redis 的分布式锁有两种，在老版本（2.6.12）之前需要两条命令才可以完成（Set 命令未完善阶段）

```shell
set lockKey lockValue
expire lockKey 30
```

​ 这两行代码中，第二行代码给当前锁设置了过期时间 30 秒，在后续的版本更新中，Redis 对 Set 命令进行了完善，即可以使用一条命令完成对锁的建立以及锁过期时间的设置。

```
SET lock_key "value" NX PX 10000
SET lock_key "value" NX EX 10
```

​ 使用 PX 的话后边跟的数字表示的是毫秒，使用 EX 则表示是秒，其中 NX 表示不存在就创建，EX 表示过期时间是 30 秒，分布式锁的核心就在于这个 NX 以及锁的名称，即 lockKey，获取锁跟释放锁的 key 必须是相同的，这样就保证了不同的线程可以遵循规矩，在同一时间仅有一个线程可以获得当前资源。

​ 另一种获得分布式锁的方式是通过 Redission，这个代码就很简单如下，这段代码中使用 tryLock 的方式加锁，自己设定了等待时间以及锁的时长。另一种设置方式是利用 Redission 自带的看门狗机制，RLock lock1 = redisson.getLock("lockKey1");，这段代码执行之后锁 key1 会获得 30 秒有效时长，然后看门狗机制会每隔 10 秒（可以自定义）来检测当前锁是否已经主动释放，如果未释放则将锁的过期时间重新设置成 30 秒，如果已经主动释放则什么也不会做。

```java
		Config config = new Config();
        config.useSingleServer().setAddress("redis://127.0.0.1:6379");
        RedissonClient redisson = Redisson.create(config);

        //获取锁的对象
        RLock lock = redisson.getLock("lockKey");
        // 尝试获取锁
        try {
            //最多等待十秒，获取锁之后三十秒自动释放
            boolean acquired = lock.tryLock(10, 30, TimeUnit.SECONDS);
            if (acquired) {
                System.out.println("Lock acquired");
                //模拟业务处理
                Thread.sleep(27000);
            }
        } catch (InterruptedException e) {
            throw new RuntimeException(e);
        }finally {
            if (lock.isHeldByCurrentThread()) {
                lock.unlock();
            }
            redisson.shutdown();  // 释放 Redisson 资源
        }
```

#### Redis 哨兵模式（Sentinel）

​ 这个模式主要出现在 Redis 集群中，哨兵机制可以监控主从节点是否按照预期工作，如果主节点失效了，哨兵会自动将一个从节点升级为主节点。哨兵是一个独立的 Redis 进程，部署在独立的服务器上。

1. 监测能力：依赖于心跳机制，哨兵每隔 1 秒向集群每个实例发送一个 ping 命令，如果没有收到回应，则认为该实例主观下线，如果超过一定数量的哨兵都认为某一个实例主管下线，那么这个时候就是客观下线了，真的下线了。

2. 哨兵选主：当主节点下线之后哨兵会自动选取一个实例作为主节点，选主规则如下：

   **优先选择数据最完整的从节点（Slave）**

   - 检查 `slave.repl_offset`（复制偏移量），即从节点同步到的 **最新数据量**，偏移量越大，数据越新，**优先成为新主节点**。
   - Redis 运行 `SENTINEL slaves mymaster` 来获取所有从节点的 `repl_offset`。

   **如果多个从节点数据相同，则选择 ID 最小的**

   - Redis 给每个节点分配了 `runid`，ID 小的节点更早加入集群，优先当选。

   **将选中的从节点提升为主节点**

   - 发送 `SLAVEOF NO ONE`，让该节点成为新的 Master。

   **让其他从节点重新复制新主节点**

   - 其他未被选中的从节点会执行 `SLAVEOF new_master_ip new_master_port`，同步新主节点的数据。

   **更新 Sentinel 配置**

   - 其他哨兵更新自己的 `mymaster` 记录，指向新的主节点。

#### Redis 的脑裂问题

​ 脑裂是指多个 Redis 节点都认为自己是主节点。比如说 A，B 两个节点，A 为主节点，但是当前 A 网络出现了暂时的波动，倒是哨兵认为 A 已经下线了，这个时候哨兵会自动选举 B 为主节点，但是过一段时间之后 A 的网络恢复了，这个时候 A 和 B 都成为了主节点，客户端还是默认和 A 连接在一起的，A 中会写入新的数据，但是 B 不会，过一段时间之后老的哨兵会将 A 节点强制降低为从节点，A 开始 B 从里转移数据，这个时候因为偏移量的问题，A 会清空自己的数据（这些数据是新的，有用的），这就导致数据不一致问题（数据库和缓存中的）。

​ **解决方案**

1. 可以设置一个参数，主节点的从节点最少是 1 个
2. 数据复制和同步的时间不能超过 5 秒，

#### Redis 的分片集群

​ 哨兵机制可以确保高可用性，分片集群可以确保大量数据的写入，分片集群的基本特征如下：

1. 集群中有多个主节点，每个主节点保存不同的数据
2. 每个主节点都有若干个从节点
3. 主节点之间通过 ping 来检测彼此的健康状态
4. 客户端请求可以访问任意节点，最终都会别转发到正确的节点
5. Redis 分片集群引入了哈希槽的概念，有 16384 个哈希槽，分配到不同的实例，根据 key 的有效部分计算哈希值，对 16384 取余
