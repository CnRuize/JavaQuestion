## Java 面试

### Java

#### Java 通识部分

##### 接口，类的多继承，类实现接口

接口可以同时继承多个接口

​ 问题 1：如果两个接口中有方法一模一样，那么只需要实现一个即可。

​ 问题 2：如果两个接口中包含相同的 default 方法，那么 Java 会报错，需要在调用的地方手动重写该方法，解决冲突。

##### Java 中序列化和反序列化

- 序列化：将一个对象转换为字节流，字节流可以保存到文件，数据库缓存中，便于传递。

  - 存储用户的设置
  - 存储未付款的订单

- 反序列化：将字节流恢复成原始对象的方式。

- 静态变量的问题

  - 如果一个类中有静态变量，新建一个对象之后，这个静态变量的值是不变的，序列化之后静态变量也不会被序列化。

  - 结论：静态变量不会被序列化，因为它们不属于实例，而是类级别的。序列化只处理对象的实例变量（非静态字段）。

    在反序列化时，静态变量会恢复为它们在类中的当前值（如果有改变，那就是改变之后的值），而不是从序列化数据中恢复。

    - 例：Person 类中静态变量 age = 10；

      ```java
      Person person = new Person（）；
      //序列化对象
      SerializableExample.serializeObject(ruize, fileName);
      //反序列化输出结果
      age = 10;
      //改变静态变量的值
      Person.age = 20;
      //反序列化输出结果
      age = 20;
      ```

##### Java 动态代理

​ Java 的动态代理可以分解为两种：JDK 动态代理和 CGLib 动态代理。由于 CGLib 是在 JDK7 之前是代理速度更快一些，因此简短描述。

**1、JDK 动态代理**

​ JDK 的动态代理是基于方法接口来实现的

```java
Proxy.newProxyInstance(JdkDynamic.class.getClassLoader(),new Class[]{ eat.class },new MyInvocationHandler());
```

​ 上述三个参数分别是"类加载器"，"代理的接口列表"，"重写的 InvocationHandler（请求处理者）"

​ 这里着重解释第三部分"InvocationHandler"，这个类似于一个"过滤器"，但是过滤的并不是请求，而是"调用的方法"，再调用方法之前会首先执行重写的 Handler 中的方法，然后再去执行我们调用的方法，类似于过滤器。

**2、CGLib 代理**

​ CGLib 代理是基于类的继承实现的，是类代理，在没有接口的时候使用，代理的速度在 JDK7 以下是比较快的。由于代理是基于继承实现的，所以不能代理 final 类跟 final 方法以及 private 方法。

##### 常见的设计模式有哪些

​ 设计模式大概可以分为三类：创建型模式、结构型模式、行为行模式

1. ​ 创建型模式：用于创建对象封装实例化的逻辑
   1. 单例模式
      - 数据库连接池
      - 线程池
   2. 工厂模式
      - 数据库连接
2. 结构型模式：处理类和对象之间的组合，增强代码复用性和灵活性
   1. 代理模式
      - Spring AOP 面向切面编程
   2. 适配器模式
3. 行为模式：用于处理对象之间的交互，提高系统灵活性
   1. 观察者模式
      - MQ 消息队列
   2. 策略模式
      - SpringSecurity 认证策略

#### Java 集合问题

![](C:\Users\Ruize\Desktop\文档\Java学习\Java八股文整理\Markdown文档图片\集合.png)

​ Java 中的集合主要分为两大类，分别是 Collection 接口和 Map 接口，前者是存储对象的集合，后者是 存储键值对。

​ Collection 接口继承 Iterable 接口，这个看源码即可，继承关系十分明显。

##### Collection 集合

1. Set（不可以有重复元素）

   - TreeSet

   - HashSet

   - LinkedHashSet

2. List（可以有重复元素）

   - ArrayList(非线程安全)

     - `ArrayList` 是一个基于数组实现的可变大小的集合类，支持快速的随机访问。

       ​ （ 频繁的修改数据也是使用 ArryayList，解释如下：

       ​ 假设我们要访问 `ArrayList` 或 `LinkedList` 中的第 1000 个元素：

       - 对于 **`ArrayList`**，我们可以通过 `array[1000]` 直接获取，这个操作是 **O(1)**。
       - 对于 **`LinkedList`**，我们必须从链表的头部开始，沿着指针逐个遍历，直到第 1000 个节点，这个操作是 **O(n)**，因为需要遍历链表中的每个节点。 )

     - ```java
       public static void main(String[] args) {
               // 创建一个ArrayList实例
               List<String> list = new ArrayList<>();

               // 向列表中添加元素
               list.add("Java");
               list.add("Python");
               list.add("C++");

               // 输出列表中的所有元素
               System.out.println("ArrayList内容: " + list);

               // 获取指定位置的元素
               String language = list.get(1); // 索引从0开始
               System.out.println("索引1处的元素是: " + language);

               // 移除元素
               list.remove("C++");
               System.out.println("移除C++后的ArrayList内容: " + list);

               // 获取列表的大小
               System.out.println("ArrayList大小: " + list.size());
           }
       ```

   - LinkedList（线程安全）

     - `LinkedList` 是基于双向链表实现的，适合频繁的插入和删除操作。

     ```java
     public static void main(String[] args) {
            // 创建一个LinkedList实例
            List<Integer> list = new LinkedList<>();

            // 向列表中添加元素
            list.add(10);
            list.add(20);
            list.add(30);
            // 获取指定位置的元素
            int num = list.get(1);
            System.out.println("索引1处的元素是: " + num);
            // 插入元素到指定位置（把15插入到第二个位置）
            list.add(1, 15);

            // 删除元素
            list.remove(Integer.valueOf(20));
            System.out.println("移除20后的LinkedList内容: " + list);
        }
     ```

   - Vector [ˈvektə] ：类似于 ArrayList 的一个动态数组，线程安全，开销大。
  
##### Map 集合

###### HashMap

![](C:\Users\Ruize\Desktop\文档\Java学习\Java八股文整理\Markdown文档图片\HshaMap常量值.png)

​ 初始化桶的数量为 16，桶的最大数量为$2^{30}$,10 亿多,默认设定负载因子是 0.75，表示有数据的桶的数量占比 75%的时候会进行扩容操作；余下几个参数和树化有关系。当一个桶内的键值对达到 8 条之后并且当前桶的数量已经到了 64（历经两次扩容，分别是有数据的桶数量有 12,24 的时候），此时桶内的数据才会由链表的形式转变为红黑树的形式。

​ 问题 1：什么样的数据会进入同一个桶？

​ 回答：哈希值相同的数据，数据进行保存前会首先进行哈希运算，得到一个哈希值，如果两个对象的哈希值相同，那么为了解决哈希冲突，HashMap 会将这两个对象放入同一个桶内。

​ 问题 2：键值对保存的对象是什么？

​ 回答：在 JDK8 中，是使用的 Node<k,v>，这个是实现的 Map.Entry<K,V>，其中包含四部分内容

```java
final int hash;
final K key;
V value;
Node<K,V> next;
```

​ 其中第一个是当前节点的哈希值，K,V 为键和值，最后一项指向链表的下一个节点（引用数据类型，占据 8 个字节）。

​ 问题 3：哈希运算是什么样子的？

​ 回答：根据不同的 key 会有不同的哈希运算方式，一般的数据类型都自己重写了 hashcode 方法，这里我看过 String 的源码，我知道 String 是如何进行哈希运算的，这个运算完成之后的结果右移动 16 位，与之前的结果相与，得到的新结果为最终的哈希值。

​ 问题 4：得到了哈希值之后该如何确定这个键值对保存在哪个桶内呢？

​ 回答：最终得到的哈希值与当前桶的总数相除，取余操作，余数是几就存入几号桶。（桶的索引是从 0 开始！）

##### 重写了 equals 方法之后是否要重写 hashcode 方法，为什么？

​ 回答：必须要重写 hashcode 方法。

​ 原因：这个用举例来说明。

​ 举例环境：比如说新建了一个类 Person，对于这个类默认的 hashcode 和 equals 方法是内存地址的整数以及内存地址。假如说只重写 equals 方法，是比较 name 的值是否相同，不重写 hashcode 方法，在此基础上新建两个对象 p1 和 p1，name 的值都是"John"。

​ 使用 HashSet 来存储当前两个对象的话，由于当前两个对象的哈希值并不是相同的，因此都会存入 HashSet 中，在机器层面，这两个对象的确是不相同的，因为他们的哈希值不相同，但是这个是与实际使用角度相背的，所及并不行。

#### Java 多线程问题

##### 如何新建多线程

1. 实现 Runnable 接口

   - 首先新建一个类实现 Runnable 接口，然后重写他的 run 方法，最后创建该类的对象，将其作为参数传递给 Thread 类的构造方法，最后调用 thread.start 方法来启动线程。

2. 实现 Callable 接口
   - 实现 callable 接口，然后传入泛型，重写 call 方法，在之后的运行中就会运行这个 call 方法而不是 run 方法，将 Mycall 的对象传递给 FutureTask（实现了 runnablefuture 接口，这个接口继承了 runnable），最后将 futureTask 对象传递给 Thread，对于结果的获取是在当前线程执行完之后才会用 get 方法得到结果。
3. 继承 Thread 类

   - 重写 run 方法，直接 new 对象就可以。

4. 线程池技术

##### 自定义线程池

1. **线程池的创建过程**
```java
ThreadPoolExecutor executor = new ThreadPoolExecutor(
    5,                       // 核心线程数
    10,                      // 最大线程数
    60L,                     // 空闲线程存活时间
    TimeUnit.SECONDS,        // 时间单位
    new ArrayBlockingQueue<>(200),  // 任务队列
    new ThreadFactory() {    // 线程工厂
        @Override
        public Thread newThread(Runnable r) {
            Thread t = new Thread(r);
            t.setName("custom-thread-" + t.getId());
            return t;
        }
    },
    new ThreadPoolExecutor.CallerRunsPolicy()  // 拒绝策略
);
```

2. **核心参数说明**
   - 核心线程数：线程池中常驻的线程数量
   - 最大线程数：线程池中允许的最大线程数
   - 空闲线程存活时间：非核心线程的空闲时间超过这个值会被回收
   - 任务队列：存储等待执行的任务
   - 线程工厂：创建新线程的工厂，可以自定义线程名称、优先级等
   - 拒绝策略：当任务队列满且线程数达到最大值时的处理策略

3. **任务队列的选择**
   - **ArrayBlockingQueue**：
     - 基于数组的有界队列，需要指定队列大小
     - 一次性分配固定大小的数组空间
     - 使用同一个锁控制入队和出队，适合生产消费速度接近的场景
     - 内存效率较高，因为是连续内存空间

   - **LinkedBlockingQueue**：
     - 基于链表的队列，可以是有界或无界
     - 动态分配节点空间，按需增长
     - 使用两个锁分别控制入队和出队，并发性能更好
     - 适合生产和消费速度差距较大的场景
     - 当设置相同大小时（如200），比ArrayBlockingQueue更灵活但内存开销更大

   - **两种队列在相同大小（如200）时的具体区别**：
     ```java
     // ArrayBlockingQueue的实现
     public class ArrayBlockingQueue<E> {
         // 生产者和消费者使用同一个锁
         final ReentrantLock lock;
         
         public void put(E e) {
             lock.lockInterruptibly();  // 添加元素时获取锁
             try {
                 // 添加元素的逻辑
             } finally {
                 lock.unlock();
             }
         }
         
         public E take() {
             lock.lockInterruptibly();  // 获取元素时也要获取同一把锁
             try {
                 // 获取元素的逻辑
             } finally {
                 lock.unlock();
             }
         }
     }
     
     // LinkedBlockingQueue的实现
     public class LinkedBlockingQueue<E> {
         // 分别使用putLock和takeLock
         private final ReentrantLock putLock = new ReentrantLock();
         private final ReentrantLock takeLock = new ReentrantLock();
         
         public void put(E e) {
             putLock.lockInterruptibly();  // 添加元素只需要获取putLock
             try {
                 // 添加元素的逻辑
             } finally {
                 putLock.unlock();
             }
         }
         
         public E take() {
             takeLock.lockInterruptibly();  // 获取元素只需要获取takeLock
             try {
                 // 获取元素的逻辑
             } finally {
                 takeLock.unlock();
             }
         }
     }
     ```

     1. **内存分配方式**：
        - ArrayBlockingQueue：创建时一次性分配200个元素的数组空间
        - LinkedBlockingQueue：创建时不会分配200个节点，而是随着元素的添加逐渐增长

     2. **锁的实现机制**：
        - ArrayBlockingQueue：使用一个锁，同时只能有一个线程操作队列
        - LinkedBlockingQueue：使用两个锁，生产者和消费者可以同时操作队列

     3. **性能表现**：
        - ArrayBlockingQueue：
          - 适合生产消费速度接近的场景
          - 单锁导致并发性能较低
          - 数组结构访问较快
        
        - LinkedBlockingQueue：
          - 适合生产消费速度差距大的场景
          - 双锁提高并发性能
          - 链表结构需要更多的内存开销

     4. **使用场景选择**：
        - 要求低延迟、内存紧张：选择ArrayBlockingQueue
        - 要求高并发、生产消费速度差距大：选择LinkedBlockingQueue

4. **线程池的运行机制**
   - 当提交任务时，优先使用核心线程
   - 核心线程满了，任务进入队列等待
   - 队列满了，创建新线程（不超过最大线程数）
   - 达到最大线程数，触发拒绝策略

5. **使用建议**
   - IO密集型任务：线程数可以多一些，如CPU核心数*2
   - CPU密集型任务：线程数一般设置为CPU核心数+1
   - 任务队列的选择要根据业务场景：
     - 要求低延迟、内存紧张：选择ArrayBlockingQueue
     - 要求高并发、生产消费速度差距大：选择LinkedBlockingQueue

### 常见的设计模式

设计模式分为三种：创建型模式，结构型模式，行为型模式

1. 创建型模式

   - 单例模式：确保一个类只有一个实例，仅提供一个全局访问点，比如需要一个记录日志的，仅仅希望被创建一次，采用静态内部类的方法来实现。

     - ```java
       // LoggerManager.java
       public class LoggerManager {

           // 私有构造方法，防止外部new
           private LoggerManager() {
               System.out.println("LoggerManager 构造方法被调用！");
           }

           // 静态内部类，懒加载 + 线程安全
           private static class Holder {
               private static final LoggerManager INSTANCE = new LoggerManager();
           }

           // 提供全局唯一访问入口
           public static LoggerManager getInstance() {
               return Holder.INSTANCE;
           }

           // 示例日志方法
           public void logInfo(String message) {
               System.out.println("[INFO] " + message);
           }

           public void logError(String message) {
               System.err.println("[ERROR] " + message);
           }
       }
       ```

       ```java
       // MainApp.java
       public class MainApp {
           public static void main(String[] args) {
               System.out.println("程序启动...");

               // 模拟模块1记录日志
               LoggerManager logger1 = LoggerManager.getInstance();
               logger1.logInfo("模块1：初始化完成");

               // 模拟模块2记录日志
               LoggerManager logger2 = LoggerManager.getInstance();
               logger2.logError("模块2：发生了异常");

               // 比较是否是同一个对象
               System.out.println("两个logger是否相同: " + (logger1 == logger2));
           }
       }
       ```

       这里涉及到了类加载重新复习了下：

       启动 MainApp → 加载 MainApp 类
       → 输出"程序启动..."
       → 调用 LoggerManager.getInstance()
       → 加载 LoggerManager
       → 访问 Holder.INSTANCE → 加载 Holder
       → 初始化 INSTANCE = new LoggerManager()
       → 构造方法执行
       → logger1.logInfo() → 输出日志
       → logger2 = getInstance() → 直接返回已存在对象
       → logger2.logError() → 输出日志
       → 比较 logger1 == logger2 → 输出 true

   - 工厂方法模式

   - 抽象工厂模式

#### 创建型模式

### JVM

#### JVM 的内存结构

JVM 在运行时将内存划分为多个区域，每个区域有不同的作用：：堆，栈，方法区，本地方法栈，程序计数器

堆：存放对象实例，这里的对象是 new 出来的对象；线程共享

方法区：类名，静态方法，静态变量；线程共享

栈：存放方法的局部变量；线程私有

本地方法栈：支持 Java 调用非 Java 代码，即执行 Native 方法；线程私有

程序计数器：记录当前线程执行的字节码指令地址，线程私有

#### Java 内存模型（Java Memory Model，JMM）

#### Java 的类加载机制

Java 采用双亲委派的方式来加载类，类加载的过程分为三部分：加载，链接，初始化。

1. 加载
   - 类加载器通过类的全限定类名找到.class 文件，读入内存，然后在方法区中创建 Class 对象
2. 链接
   1. 验证：确保 class 文件没有安全问题
   2. 准备：给静态变量分配内存，并设置对应的初始值，比如 int 设置 0，Integer 对应的是 null
   3. 解析：将类方法字段的符号引用转换为直接引用，即内存地址
3. 初始化
   - 赋予静态变量最终值，并执行 static 代码块

问题：双亲委派机制

回答：当一个类加载器接收到类加载请求时，并不会自己去加载类，而是先委托其父类加载器进行加载，只有父类加载器无法完成当前任务的时候才会自己主动去加载。

​ 在 JDK8 中的类加载器层级结构如下：自定义 ClassLoader——>APPClassLoader(应用类加载器)——>ExtClassLoader(扩展类加载器)——>BootStrapClassLoader（启动类加载器）

- 作用：

  - 防止用户自定义的类覆盖核心类
  - 同一个类只会被类加载器加载一次，减少内存的浪费

- 举例：假如需要加载 String 类

- ↓ 自定义 ClassLoader 收到请求
  ↓
  先委托给 AppClassLoader（应用类加载器）
  ↓
  再委托给 ExtClassLoader（扩展类加载器）
  ↓
  再委托给 BootstrapClassLoader（启动类加载器）
  ↓
  Bootstrap 找到 rt.jar 中的 String.class → 加载成功，返回结果

  ✅ 自定义 ClassLoader 什么都没做，只有在父类都加载失败时，自定义的类加载器才会起作用。

#### Java 的垃圾回收机制

主要作用区域：堆；因为堆中存放了实例化对象。

对象分为新生代，老年代（存活时间长，较大的对象），新生代中分为三种区：Eden（对象刚存放时创建），Survivor From 区（存活对象），Survivor To 区（存活对象），如果是一个大的对象会直接进入老年代。

**1、判断对象是否可以被回收**

1. 引用计数法（已淘汰）
2. 根可达性分析

**2、垃圾回收算法**

1. 标记清除法

   1. 标记存活对象，清除未标记对象
   2. 产生大量内存碎片，影响内存分配

2. 标记整理法

   1. 标记存活对象，整理存货对象到一边，清理无效空间
   2. 老年代常用，减少内存碎片

3. 复制算法

   1. 将存活对象从 Eden 区复制到 Survivor 区
   2. 清空 Eden 区

   特点：新生代常用，老年代不适用，大对象复制代价较大

**3、垃圾回收器**

| **垃圾回收器**                            | **算法**  | **特点**                   | **适用场景**            |
| ----------------------------------------- | --------- | -------------------------- | ----------------------- |
| **Serial**                                | 复制      | 单线程，暂停时间长         | **单核 CPU**            |
| **ParNew**                                | 复制      | Serial 的多线程版本        | **多核 CPU**            |
| **CMS（Concurrent Mark-Sweep）**JDK8 默认 | 标记-清除 | 并发收集，低延迟，但有碎片 | **低延迟应用**          |
| **G1（Garbage First）**                   | 标记-整理 | 分区回收，低延迟           | **大内存、高吞吐**      |
| **ZGC**                                   | 标记-整理 | 低延迟，支持 TB 级内存     | **JDK 11+，高性能系统** |

**4、MinorGC 和 FullGC 有什么区别，如何避免 FullGC**

MinorGC 是新生代满了，只清理新生代，速度快频率高。

FullGC 是老年代和方法区满了，清理整个堆和方法区，影响大且会发生 STW，所有线程都会被迫停止

避免：增大堆的大小，垃圾回收期使用 G1 或者是 ZGC，优化对象生命周期，减少老年代对象，避免频繁调用 System.gc（）

### Spring

#### SpringMVC 的执行流程

1. 客户端向服务器发出 Http 请求之后，首先会被 DispatcherServlet 接收
2. DispatcherServlet 会调用 HandlerMappering 根据请求的地址来找到处理该请求的 Controller
3. 通过 HandlerAdapter 调用具体的 Controller
4. Controller 业务处理
5. 处理完成之后返回 ModelAndView 对象，如果加了 ResponseBody 则是直接返回 JSON 数据。
6. 最后分为两种情况，一是返回的 ModelAndView，这个时候 DispatcherServlet 会调用 ViewResolver 来解析 View 数据，并把 model 数据填充到视图中；如果返回的是 Json 对象的话则直接返回给前端即可

#### Spring 的 AOP 跟 IOC

- IOC（控制反转）是 Spring 框架的核心思想之一，指的是**对象的创建和依赖关系的管理从程序员手中"反转"给了 Spring 容器来完成。**

  以整合 MyBatis 为例：

  - 在传统开发中，我们需要自己创建 `SqlSessionFactory`、打开 `SqlSession`、编写实现类等。
  - 而在 Spring 中，我们只需要在配置文件中定义好数据源，Spring 会自动帮我们创建并管理 `SqlSessionFactory`、`SqlSession`，并在合适的时机进行销毁。
  - 对于加了 `@Mapper` 注解的接口，Spring 会通过 MyBatis 的代理机制，自动为其生成实现类（动态代理对象），并将其注入到我们需要使用的地方，例如在 `Service` 中通过 `@Autowired` 进行依赖注入。

- AOP：面向切面编程，可以再具体方法前后执行我们自定义的特殊方法

  - 使用方法

    1. 使用切点表达式 PointCut 确定切点的方法
    2. 使用**`@Before`**：方法执行之前执行**`@After`**：方法执行之后执行，无论方法是否抛出异常。**`@AfterReturning`**：方法正常执行之后执行，并获取返回值。**`@AfterThrowing`**：方法抛出异常时执行，并获取异常信息。**`@Around`**：最强大的注解，可以控制目标方法执行的整个过程，包括在方法执行前、后以及修改返回值等。

  - 示例如下

  - ```java
    @Component
    @Aspect
    public class LoginAspect {
        //1、切点，匹配passwordLogin方法
        @Pointcut("execution( * com.ruize.todolistback.controller.UserController.passwordLogin(..))")
        public void passwordLoginPoint() {}
        //2、方法执行后执行
        @AfterReturning(pointcut = "passwordLoginPoint()" ,returning = "result")
        public void afterLogin(JoinPoint joinPoint, Result<String> result) {
            System.out.println("返回值：" + result);
        }
    }
    ```

#### 什么叫做依赖注入

"依赖"就是一个类在工作时**需要使用另一个类的功能**。

比如你写一个 `UserService`，里面要访问数据库的用户数据，你就**依赖** `UserMapper`：

```java
public class UserService {
    private UserMapper userMapper;
}
```

在传统写法中我们可能需要 new 一个对象出来，但是在 Spring 框架中直接利用 Spring 的控制反转之后用 Autowired 自动注入就可以了

### Mybatis

#### Mybatis 如何防止 SQL 注入

使用#代替$符号

### Mysql

#### Mysql 如何定位慢查询

1. 可以使用运维工具比如 Skywalking，可以看到具体某个接口执行情况
2. 使用 Mysql 慢日志的查询，设置 SQL 执行时间，比如说当前 sql 执行超过两秒就记录当前 sql

#### SQL 执行很慢如何优化

1. 是否出现了聚合查询，多表查询，查询的数据量是不是很大？

2. 可以使用 Explain 字段来分析对应的 sql，返回的结果中会有几个需要关注的字段

   1. key 和 possible_key，这可以确定当前 sql 使用了哪个索引

   2. extra 字段，这是 mysql 自己给出的建议，这里可以看出当前的 sql 是否需回表

      | extra 内容      | 含义解释                                         |
      | --------------- | ------------------------------------------------ |
      | Using index     | 使用了**覆盖索引**，不需要回表                   |
      | Using where     | 有使用 `WHERE` 条件过滤                          |
      | Using temporary | 用了临时表，通常和 `GROUP BY` 或 `ORDER BY` 有关 |
      | Using filesort  | 表示排序是**通过外部排序实现的**，不是索引排序   |
      | **NULL**        | 表示没有额外操作，通常是个好事（查询很干净）     |

   3. type 字段表示的是 Mysql 访问数据的方式性能从好到坏一般为：

      ```sql
      system > const > eq_ref > ref > range > index > all
      ```

      1. system：表示只有一行数据
      2. const：通过主键或者是唯一索引等值查询，直返会一条结果
      3. eq_ref：每次扫描主表时，子表通过主键或者是唯一索引精确匹配一条记录
      4. ref：通过非唯一索引匹配到若干条记录
      5. range：使用了范围查询
      6. index：遍历了整个索引，索引的全扫描
      7. all：全表扫描，

#### Mysql 索引

##### 索引是什么？

​ 索引是为了提高查询效率所建立的一种数据结构组织形式，在不同的数据库中有不同的表现方式，比如说在 Mysql 数据库中常见的 Innodb 是采用 B+树的方式实现的，对于旧版本的 MyISAM 引擎也曾采用 B+树来实现，但是聚簇机制不同。

##### 索引的基本类型

Mysql 索引类型包含以下几种：

1. 主键索引（字段值不可以重复）

2. 普通索引（索引字段的值可以重复）

3. 唯一索引（字段值不可以重复）

4. 联合索引（两个字段一起作为一个索引）

   河北+小明 != 安徽+小明

5. 前缀索引（取当前字段的前几个字母）

6. 全文+空间索引（暂时用不到，忽略）

只有主键索引在 B+树中是保存了完整的行数据。

##### 索引覆盖

要查询的字段内容是 B+树叶子结点的一部分

- ​ 比如说我们查询全部内容采用主键索引作为依据，这个就是索引覆盖，因为主键索引对应的叶子结点是整行完整数据。
- ​ 如果采用非主键索引，得到结果是只有当前行的主键以及普通索引的值，如果我们要的字段没有那么就要回表

#### 索引创建的原则

- 针对数据量特别大的表（10 万条数据以上）；
- 经常作为查询条件的字段，一个或者是多个；
- 在区分度高的字段上创建索引，可以仅仅通过索引就可以定位具体的信息
- 如果是字符串形式的比较长，可以用前缀索引，取字符串的前几个字符
- 尽量使用联合索引——>便于索引覆盖
- 索引并不是越多越好——>上楼（步行，电梯，飞机）方式举例

#### 索引失效

- 违反最左索引原则（这个情况只会在多字段索引出现）
  - 如果遵从最左索引，但是跳跃了某一列，这样的话就只有最左索引生效
- 范围查询右边的列，不能使用索引
  - 比如果三个字段的索引，第一个字段是"="，第二个字段是">"，第三个字段不管是什么都会失效
- 不能在索引上进行运算操作
  - 比如说对姓名字段进行截取
- 类型转换失败
  - 当前字段是数字类型，传递值是字符串类型呢
- 以%开始的模糊匹配
  - 如果是放在最后边做模糊匹配的话就可以

#### sql 优化经验

1. select 禁止使用 select \*（为了使用覆盖索引，降低回表查询）
2. SQL 语句尽量不要写成索引失效的方法
   1. 避免在 where 字句中对字段进行表达式操作
3. 在联合查询结果的时候使用 union all 来代替 union
   1. 使用 union all 会直接返回两个结果的合集
   2. 使用 union 的话会将重复部分仅返回一个
4. 在进行连接查询的时候尽量使用内连接，而不是用左右连接
   1. 内连接仅仅会返回两个表中都满足匹配标准的内容
   2. LEFT/RIGHT JOIN 会保留主表所有记录，若副表存在一对多关系，极易造成结果集膨胀，甚至影响后续分页、聚合逻辑。

#### mysql 为什么使用 Innodb 作为数据库引擎

1. 支持事务（ACID，原子性，一致性，隔离性，持久性）
   1. 这里就用存钱取钱的例子来记忆，原子性表示这个过程不可以分割，比如说存钱取钱一起作为一个事务，那么就表示要么一起完成要么就失败回滚
2. 行级锁
   1. 相对于 Myisam 而言，行级锁比表级锁的性能要更高
3. MVCC 多版本并发控制
4. 崩溃恢复
   1. 基于重做日志（redo log）以及回滚日志（undo log）
5. 支持外键
6. 聚簇索引

#### Mysql 的超大分页处理

​ 分页查询常使用 `LIMIT A, B` 语法，表示从第 A 条数据开始，查询 B 条记录。其本质执行过程决定了性能的好坏，尤其是在数据量很大时。首先需要了解一下这个过程为什么会耗费资源，在默认情况下 limit 会将前 n 条数据都缓存在内存中，如果有索引的话那么缓存的数据是 B+树路径上的所有数据，数据还少一些。

- 换成 where id（主键）> n limit 50 的方式

  - 主键范围查询可以实现从 B+树中快速定位所要的数据位置，不需要把数据缓存在内存

- 使用主键的 order by 来操作

  - 这个优化不如上边，主要消耗在线性遍历链表，这里需要了解 sql 的执行流程，索引的 order by 来举例：

  - ```sql
    SELECT * FROM user ORDER BY id LIMIT 90000, 50;
    ```

  - 这里因为 id 是主键，Mysql 会从 B+树种由根节点找到最左叶子结点，然后通过叶子结点之间的双向链表来进行链表的线性遍历，直到 90000 这个数据，然后往后读取 50 条，返回结果

#### Mysql 查询

##### 聚合查询

聚合查询通过 group by 来实现，目的是统计某一项信息

```sql
-- 统计每个用户的订单总金额
SELECT user_id, SUM(order_amount)
FROM orders
GROUP BY user_id;
```

聚合索引会查询很慢往往是因为数据量很大，Mysql 内存临时表放不下，然后要写入磁盘，这会导致性能极致下降。

##### 多表查询

通过键的联合来形成一张逻辑表

```sql
-- 查询每个订单的用户姓名和订单金额
SELECT o.id, o.amount, u.username
FROM orders o
JOIN users u ON o.user_id = u.id;
```

多表查询+聚合查询示例

```sql
-- 查询每个用户的总消费金额（需要先关联订单再聚合）
SELECT u.username, SUM(o.amount) AS total_spent
FROM users u
JOIN orders o ON u.id = o.user_id
GROUP BY u.id, u.username;
```

##### 深度分页查询

深度分页查询就是分页去查询很后边的数据。

```sql
SELECT *
FROM your_table
ORDER BY id
LIMIT 89990, 10;
```

上述 sql 就是从 89990 条数据开始，查询十条数据。

#### Mysql 事务的特性

​ ACID

1. 原子性
2. 一致性
3. 持久性
4. 隔离性

#### Mysql 事务隔离级别

##### 问题引入：并发事务带来哪些问题？

​ 脏读，不可重复，幻读

​ 脏读：一个事务读取到另一事务还未提交的数据

​ 不可重复度：一个事务先后读取同一条数据，两次读取数据不同

​ 幻读：一个事务在查询时没有查询到数据，但是插入数据时显示当前数据已经存在

##### Mysql 事务隔离级别解决并发事务问题

​ 事务隔离级别有读未提交，读已提交，可重复读，串行化

​ 读未提交：什么也解决不掉

​ 读已提交：解决脏读问题

​ 可重复读：解决了脏读和不可重复读的问题；这是 Mysql 默认的事务隔离级别

​ 序列化：全部解决，性能降低

#### 默认的隔离级别可重复读是如何实现的？

#### undo log 和 redo log 的区别

undo log 记录的是逻辑日志，主要用于事物的回滚，支持事务的原子性

redo log 记录的是数据物理页面的变化，服务器宕机之后可以用来恢复数据

​ 在数据写入的时候 Inoodb 会先写入 redo log 然后再写入磁盘

#### MVCC 是什么？

MVCC 是 Mysql 提供的一种多版本控制的机制

### Redis

#### Redis 为什么这么快

1. Redis 将数据存储在内存中，内存速度比磁盘快。
2. Redis 提供高效的数据结构。
3. Redis 使用单线程事件驱动模型，结合**IO 多路复用**，避免了多线程上下文切换条件，提高了并发处理效率。

1、**I/O 多路复用？**

​ Redis 是纯内存操作，执行速度很快，瓶颈在于网络延迟，I/O 多路复用主要实现高效的网络请求。IO 多路复用是用单个线程来同时监听多个 Socket，并在某个 Socket 可读可写的时候得到通知，避免无效的等待。

1. 同步阻塞：逐个收作业，A 没做完就等 A 做完再收其他人的。
2. 同步非阻塞：逐个收作业，不过 A 没做完就暂时不管 A，先收别人的。
3. select/poll：有人做完作业会举手，但是你并不知道谁举手，需要一个一个问。
4. epoll：有人做完就举手，你也知道谁做完了。

#### Redis 可以保存多少种数据类型（5 种常见+3 种特殊）

1. String

2. Hash

   - redis 内部使用哈希表来实现

   - 其实存储的就是键值对，不过这个值可以是数组类型的，像一个类一样。

     ![Redis哈希值](C:\Users\Ruize\Desktop\文档\Java学习\Java八股文整理\Markdown文档图片\Redis哈希值.png)

3. List

   ![List类型数据](C:\Users\Ruize\Desktop\文档\Java学习\Java八股文整理\Markdown文档图片\List类型数据.png)

4. Set

   和一般的 Set 没什么区别，都是没有重复数据，不过对应的方法有所变化。

5. ZSet

   数据在插入时都会附带一个参数，数据的顺序按照，按照分数从低到高排序。

   ![set](C:\Users\Ruize\Desktop\文档\Java学习\Java八股文整理\Markdown文档图片\set.png)

   6.BitMap 位图

   存储的也是键值对，不过键是分级的，值的保存形式是一个字符串，设置好偏移位，并设置对应的值 1 或者是 0 即可

   setbit ruize：20250510 1 1；键是 ruize，字键是 20250510 ，偏移位 1，值是 1

   7.Hyperloglog

   内从占用低，即使有 1 亿个数据，内存占用也是 12kb，不能查看其中的内容，只是可以数据统计得到其中有多少唯一的内容

   8.Stream 流

   有点类似于消息队列

   ```shell
   # 1. 创建Stream并添加事件（无需预先声明Stream）
   XADD payment_events * user_id "101" amount "99.99" status "pending"

   # 2. 创建消费者组（绑定到payment_events流）——>某个流下包含多个组
   XGROUP CREATE payment_events payment_group $

   # 3. 消费者A从组中读取事件
   XREADGROUP GROUP payment_group consumerA COUNT 1 STREAMS payment_events >

   # 4. 处理完成后确认
   XACK payment_events payment_group 1715581234567-0
   ```

#### Redis 缓存穿透，雪崩，击穿

1. 缓存穿透：查询一个不存在的数据，导致请求访问数据库，
   - 解决：
     - 缓存这个不存在的数据
     - 使用布隆过滤器，过滤器中包含数据库中的全部内容
       - 存储数据时，通过多个哈希函数来进行运算，把数组中对应的位置改为 1，在进行查询的时候，对查询数据进行哈希运算，检查结果位置是否全部为 1.
2. 缓存雪崩：大量缓存数据在同一时间过期导致请求大量访问数据库
   - 解决
     - 使用随机过期时间
     - 更新频率提高，不要等到缓存过期采取更新
3. 缓存击穿：某个热点数据过期导致请求大量访问数据库
   - 解决
     - 热点数据永远不过期
     - 使用互斥锁，确保同一时间只有一个请求可以去数据库查询数据

#### Redis 主从复制

​ 从节点向主节点发送 Psync 命令来建立一个链接

**1、全量同步**

​ 如果是第一次连接或者是之前的连接失效，主节点会将数据快照 RDB 全部发送给从节点

​ 1.1 过程解析

​ 从节点发送 psync replicationID offset 给主节点，replicationID 为服务器当前的的 replicationID ，由于第一次同步并不知道主服务器的 replicationID 是多少，所以这里是" ？"，而对于 offset 复制进度来讲，第一次同步的值为-1，所以综上所述第一次从节点进行全量同步时，发送的命令为 psync ？ -1；主节点收到命令之后发现 replicationID 没有值，于是确定本次复制为全量复制，主节点会生成 RDB 文件，在 RDB 文件生成过程中如果发生了新的写的操作，把新的写操作保存在 repilcation buffer 中等待从节点 RDB 文件加载完成之后把 replication buffer 文件中存储的数据发送给从节点，从节点继续把数据写入，实现数据一致性。

**2、增量同步**

​ 全量复制完毕之后，主节点和从节点之间还会保持一个长连接，当主节点完成写操作之后，会相应的更新从节点

​ 2.1 过程解析

​ 当全量同步一次之后断开了，此时再进行全量同步的话消耗比较大。repl_backlog_buffer：环形缓冲区，默认大小为 1M，主节点会将命令缓存到当前的缓冲区，但是超过 1M 之后，之前的数据就会被覆盖。对于增量同步而言，命令也是 psync，如果主节点根据 replicationID 判断和主节点一致而且根据 offset 判断数据还在 repl_backlog_buffer 中就进行增量同步，在 repl_backlog_buffer 中的数据写入到 replicaiton buffer 中，最后将 replication buffer 写入给子节点，一次增量同步完成。

**3、repl_backlog_buffer 和 replication buffer 的区别**

​ repl_backlog_buffer 仅仅有一份但是 replication buffer 对于每一个子节点都有一份。

​ 3.1 优点

​ 主从复制可以实现读写分离，写数据操作在主节点实现，读操作在从节点实现。

**4、问题**

1. 如果刚在主节点写好，还未来得及同步给从节点，这个时候从节点是读取不到数据的，要访问数据库了吗？
1. 不要，直接返回 null 值

#### Mysql 中的数据如何与 Redis 进行同步呢？（双写一致性）

一致性要求高

​ 1、采用写锁的方案来实现，当用户查询 Redis 中发现不存在的时候，查询数据库，得到结果之后先获取写锁，保证只有当前线程可以对 Redis 进行读取和写数据的操作，保证数据一致性。

​ 2、采用 canal 来实现，canal 是阿里巴巴开源的中间件，他会解析"Binary log"记录 Mysql 数据库中的变化，解析其中的内容，然后直接调用 Redis 的 API 更新其中的数据，实现强一致性。

双写一致性基本方案有如下几种：

1. 先更新缓存，再更新数据库

   1. A 线程更新缓存，下一步更新数据库，此时 B 线程查询缓存，得到正确数据，下一步 A 线程更新数据库，最终结果是得到的数据是正确的，缓存跟数据库中的数据也是正确的。
      1. 异常情况：数据库更新失败，B 第一次得到的数据是正确的，但是如果 A 更新数据失败，比如说死锁或者是遇到了网络问题，Mysql 数据回滚了，这样就会导致缓存中是异常的值，与 Mysql 中的数据并不一致。

2. 先删除缓存，再更新数据库

   1. A 线程删除缓存，下一步更新数据库，此时 B 线程查询不到缓存来查询数据库，得到的旧的数据，返回更新缓存，缓存中的数据是旧的数据，A 回头来更新数据库，最终结果是数据库内容正确，缓存内容不正确。

3. 先更新数据库，再更新缓存

   1. 存在脏数据问题，数据库中数据已经更新了，缓存还没来得及更新，此时第二个线程此时查询的是缓存中的数据，脏数据问题，最后更新缓存，最终结果是数据库和缓存都是正确的，但是在这期间出现了脏数据的问题。

4. 先更新数据库，再删除缓存（推荐 √）

   1. 存在脏数据问题，数据库中数据已经更新了，缓存还没来得及删除，此时第二个线程此时查询的是缓存中的数据，脏数据问题，最后重新建立缓存，最终结果是数据库缓存都正确，但是期间出现了脏数据的问题。

5. 先删除缓存，更新数据库，删除缓存（延迟双删）

   执行流程：删除缓存，更新数据库，延时 300-800ms 再次删除缓存

   ​ 1、A 线程删除缓存，该更新数据库了，B 线程查询数据得到脏数据，更新缓存，A 线程更新数据库，延时删除缓存，最终结果是数据库内容正确，但是过程中 B 线程得到了脏数据。

   ​ 2、A 线程删除缓存，更新数据库，此时 B 来查询就是得到的正确数据，A 删除缓存，最终结果就是全对。

​ 6.结合消息队列（延时双删改版）——>最终一致性，还是存在脏数据问题

​ 第二次删除使用消息队列发送消息给 Redis，确定多长时间之后进行缓存的删除，同时可以设置消息重传机制以及死信队列。

​ 7.采用 redis 的分布式锁来实现强一致性 ——>强一致性

​ redis 的分布式锁控制的是线程先后顺序，可以确保在拿到锁的时间段内，只有当前线程可以操作。

​ 8.采用阿里巴巴开源组件 cannal

​ 这是一种强一致性的方案，这是一个基于 MySQL binlog 解析的增量数据订阅&同步组件，Canal 模拟成 Mysql 的从库，监听 Mysql 的 binlog 日志，实时捕获数据变更并更新到 Redis，使用要求有以下几点：

1. Mysql 必须要开启 Binlog 日志，格式必须要是 Row 格式
2. Canal 解析 Binlog 文件，解析数据变更。
3. Canal 触发回调，将变更同步到 Redis
4. Redis 与 Mysql 保持数据一致

使用方法有点难，搞了半天没弄成。

​

​ 读锁：其他线程可以读数据，但是无法写。

​ 写锁：其他线程不可以读也不可以写。

​ 分布式锁：分布式锁是 redis 专属的一种锁，线程首先获取锁，因为 SET NX PX 的原因，其中 NX 表示不存在，如果不存在就就获取锁，存在就放弃，所以这就可以确保只有一个线程可以操作。

#### Redis 持久化

1. AOF

   Append-Only File Redis 执行的每一条命令都会写入到 AOF 文件中，

   ​ 优点：数据完整性好，可以通过配置来决定 AOF 的写入频率

   ​ 缺点：写操作频繁，性能比较差

2. RDB

   Redis DataBase Backup file Redis 数据快照，把内存中的数据都存储在磁盘中，当数据恢复时会从磁盘读取所有数据。

   Redis 在生成快照的时候会采用 fork 机制，开启一个子进程执行持久化，主进程不会受到阻塞。

   ​ 缺点：生成快照之前的数据会丢失。

   RDB：比如 300 秒内写入 5 次就进行 RDB，这里的时间间隔是距离上一次 RDB 完成之后的时间

#### 数据的删除策略

1. 惰性删除：使用 key 时会检测其是否过期，过期的话就删除，没过期就拉倒
2. 定期删除：每隔一段时间就对一定量的 Key 进行检测，过期的删除，没有过期不管

#### Redis 数据淘汰策略

1. 默认策略是不淘汰任何 key，内存满的时候不允许写入数据
2. volatile-ttl（ [ˈvɒlətaɪl]）：TTL 剩余值越小的先被淘汰
3. allkeys-random：所有的 key 随机淘汰
4. volatile-random:对设置过期时间的 key 随机进行淘汰
5. allkeys-lru:对全体 key 使用 LRU（Least Recently Used 最近最少未使用）算法淘汰：用当前时间减去最后一次访问时间，值越大越先被淘汰
6. volatile-lru：对设置了过期时间的 key 采用 lru 算法
7. allkeys-lfu:对全体 key 使用 LFU（Least Frequently Used 最少频率使用）会统计每个值的访问频率，值越小，越先被淘汰。
8. volatile-lfu：对设置过期时间的 key 采用 lfu 算法进行淘汰

#### Redis 的分布式锁

​ 分布式锁：在锁存在期间，仅有当前获得锁的线程可以进行输出读写操作。

​ Redis 的分布式锁有两种，在老版本（2.6.12）之前需要两条命令才可以完成（Set 命令未完善阶段）

```shell
set lockKey lockValue
expire lockKey 30
```

​ 这两行代码中，第二行代码给当前锁设置了过期时间 30 秒，在后续的版本更新中，Redis 对 Set 命令进行了完善，即可以使用一条命令完成对锁的建立以及锁过期时间的设置。

```
SET lock_key "value" NX PX 10000
SET lock_key "value" NX EX 10
```

​ 使用 PX 的话后边跟的数字表示的是毫秒，使用 EX 则表示是秒，其中 NX 表示不存在就创建，EX 表示过期时间是 30 秒，分布式锁的核心就在于这个 NX 以及锁的名称，即 lockKey，获取锁跟释放锁的 key 必须是相同的，这样就保证了不同的线程可以遵循规矩，在同一时间仅有一个线程可以获得当前资源。

​ 另一种获得分布式锁的方式是通过 Redission，这个代码就很简单如下，这段代码中使用 tryLock 的方式加锁，自己设定了等待时间以及锁的时长。另一种设置方式是利用 Redission 自带的看门狗机制，RLock lock1 = redisson.getLock("lockKey1");，这段代码执行之后锁 key1 会获得 30 秒有效时长，然后看门狗机制会每隔 10 秒（可以自定义）来检测当前锁是否已经主动释放，如果未释放则将锁的过期时间重新设置成 30 秒，如果已经主动释放则什么也不会做。

```java
		Config config = new Config();
        config.useSingleServer().setAddress("redis://127.0.0.1:6379");
        RedissonClient redisson = Redisson.create(config);

        //获取锁的对象
        RLock lock = redisson.getLock("lockKey");
        // 尝试获取锁
        try {
            //最多等待十秒，获取锁之后三十秒自动释放
            boolean acquired = lock.tryLock(10, 30, TimeUnit.SECONDS);
            if (acquired) {
                System.out.println("Lock acquired");
                //模拟业务处理
                Thread.sleep(27000);
            }
        } catch (InterruptedException e) {
            throw new RuntimeException(e);
        }finally {
            if (lock.isHeldByCurrentThread()) {
                lock.unlock();
            }
            redisson.shutdown();  // 释放 Redisson 资源
        }
```

#### Redis 哨兵模式（Sentinel）

​ 这个模式主要出现在 Redis 集群中，哨兵机制可以监控主从节点是否按照预期工作，如果主节点失效了，哨兵会自动将一个从节点升级为主节点。哨兵是一个独立的 Redis 进程，部署在独立的服务器上。

1. 监测能力：依赖于心跳机制，哨兵每隔 1 秒向集群每个实例发送一个 ping 命令，如果没有收到回应，则认为该实例主观下线，如果超过一定数量的哨兵都认为某一个实例主管下线，那么这个时候就是客观下线了，真的下线了。

2. 哨兵选主：当主节点下线之后哨兵会自动选取一个实例作为主节点，选主规则如下：

   **优先选择数据最完整的从节点（Slave）**

   - 检查 `slave.repl_offset`（复制偏移量），即从节点同步到的 **最新数据量**，偏移量越大，数据越新，**优先成为新主节点**。
   - Redis 运行 `SENTINEL slaves mymaster` 来获取所有从节点的 `repl_offset`。

   **如果多个从节点数据相同，则选择 ID 最小的**

   - Redis 给每个节点分配了 `runid`，ID 小的节点更早加入集群，优先当选。

   **将选中的从节点提升为主节点**

   - 发送 `SLAVEOF NO ONE`，让该节点成为新的 Master。

   **让其他从节点重新复制新主节点**

   - 其他未被选中的从节点会执行 `SLAVEOF new_master_ip new_master_port`，同步新主节点的数据。

   **更新 Sentinel 配置**

   - 其他哨兵更新自己的 `mymaster` 记录，指向新的主节点。

#### Redis 的脑裂问题

​ 脑裂是指多个 Redis 节点都认为自己是主节点。比如说 A，B 两个节点，A 为主节点，但是当前 A 网络出现了暂时的波动，倒是哨兵认为 A 已经下线了，这个时候哨兵会自动选举 B 为主节点，但是过一段时间之后 A 的网络恢复了，这个时候 A 和 B 都成为了主节点，客户端还是默认和 A 连接在一起的，A 中会写入新的数据，但是 B 不会，过一段时间之后老的哨兵会将 A 节点强制降低为从节点，A 开始 B 从里转移数据，这个时候因为偏移量的问题，A 会清空自己的数据（这些数据是新的，有用的），这就导致数据不一致问题（数据库和缓存中的）。

​ **解决方案**

1. 可以设置一个参数，主节点的从节点最少是 1 个
2. 数据复制和同步的时间不能超过 5 秒，

#### Redis 的分片集群

​ 哨兵机制可以确保高可用性，分片集群可以确保大量数据的写入，分片集群的基本特征如下：

1. 集群中有多个主节点，每个主节点保存不同的数据
2. 每个主节点都有若干个从节点
3. 主节点之间通过 ping 来检测彼此的健康状态
4. 客户端请求可以访问任意节点，最终都会别转发到正确的节点
5. Redis 分片集群引入了哈希槽的概念，有 16384 个哈希槽，分配到不同的实例，根据 key 的有效部分计算哈希值，对 16384 取余

### 消息中间件

#### RabbitMQ

​ 支持强事务，有消息确认机制，可以确保消息不丢失，但是默认情况下消息投递之后就会被删除，无法恢复，吞吐量较低为万 TPS，延迟低，毫秒级别

​ **RabbitMQ**：生产者 ➝ 交换机 ➝ 队列 ➝ 消费者（消息是一次性消费的）

#### Kafka

​ 高吞吐量，百万 TPS（Transactions Per Second 每秒处理业务数量），支持消息持久化，可以回溯消费，但是默认不支持强事务，需要手动幂等处理来防止消息丢失

​ **Kafka**：生产者 ➝ 主题 ➝ 分区 ➝ 消费组（消息可以被多个消费组消费）

### 数据结构

#### 红黑树

​ 二叉树搜索树的一种（左<根<右（值的大小））

1. 每个节点是红色或者是黑色
2. 根节点必须是是黑色的
3. 没有两个连续的红色节点
4. 从根节点到所有叶子结点经过的黑色节点数量相同
5. 最长路径不会超过最短路径的 2 倍
6. 红黑树在插入删除更高效
7. 插入节点默认是红色，插入之后需要进行调整，根据不同情况不同判断。

#### B+树

b+树是多路平衡查找树，

- 所有的数据存储在叶子结点
- 非叶子节点只做索引，不存储数据
- 叶子结点通过链表相连接

### 计算机网络

### 操作系统

#### 进程和线程的区别

1. 进程可以包含多个线程
2. 进程是操作系统分配资源的最小单位，线程是 CPU 调度的基本单位
3. 进程有独立的数据，堆，栈；线程共享进程的数据，堆，栈
4. 调度方面，进程切换耗费大，线程很小
5. 一个进程崩溃不会影响其他进程，但是某个线程崩溃会影响其他线程，可能导致进程失败

#### 并发和并行的区别

1. 并发：多个任务在某一个时间段内切换交替执行
   1. 共享单个处理器资源
2. 并行：多个任务同时执行
   1. 共享多个处理器资源

### Docker

#### 镜像，容器概念

使用 docker pull 拉取到本地的是镜像

```shell
docker pull nginx
```

执行完此行代码之后可以使用 **docker images** 查看本地所有的镜像

运行镜像会产生一个对应的容器，一个镜像可以多次运行，产生不同的容器，端口对应好就可以了，不同的容器运行方式不同。

```shell
docker run -d --name my-nginx -p 8080:80 nginx
```

- `docker run` 会基于镜像创建并运行一个新的容器。
- `--name my-nginx` 给容器起一个名称（方便管理）。
- `-p 8080:80` 将主机端口 8080 映射到容器内部的 80 端口。
- `-d` 后台运行。

```shell
docker run -d --name my-mysql \
  -e MYSQL_ROOT_PASSWORD=root \
  -p 3306:3306 \
  mysql
```

- `-d`：后台运行容器。
- `--name`：指定容器名称为 `my-mysql`。
- `-e MYSQL_ROOT_PASSWORD=root`：设置 root 用户的密码。
- `-p 3306:3306`：将主机的 3306 端口映射到容器的 3306 端口。

在运行完上述内容之后执行 docker ps 会看到当前运行的容器，使用`docker stop my-mysql`可以停止，再次使用`docker start my-mysql`即可重新启动。

如果要删除`容器`使用的是`docker rm name`，比如 docker rm my-nginx；

如果要删除`镜像`，使用的是`docker rmi name ` 比如 docker rmi mysql

#### 数据卷(Volume)

数据卷：是 Docker 提供的一种**独立于容器生命周期的持久化存储机制**，用于将数据存储在宿主机指定目录或 Docker 管理的目录中，从而实现容器之间数据共享或持久化。

​ 数据卷默认挂载位置：/var/lib/docker/volumes

​
